# -*- coding: utf-8 -*-
"""RandomForest and Adaboost Full Dataset Confusion Matrix.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iDc8PJ738jKZvzjHPrYBXD2D2AgUPbXn
"""

#importing Libraries
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV, train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
import umap.umap_ as umap
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Importing dataset
dataset = pd.read_csv("diabetes_012_health_indicators_BRFSS2015.csv")
pd.set_option('display.max_columns', None)  # to make sure you can see all the columns in output window
print(dataset.head())
print(dataset.shape)
print(dataset.info())
print(dataset.describe())

# Map Diabetes_012 to binary classification
dataset['Diabetes_012'] = dataset['Diabetes_012'].map({0: 0, 1: 0, 2: 1})
dataset.info()

# Random sampling to reduce dataset size for parameter tuning
dataset_sample = dataset.sample(frac=0.2, random_state=101)

# Dividing dataset into label and feature
X_sample = dataset_sample.drop('Diabetes_012', axis=1)  # Features
Y_sample = dataset_sample['Diabetes_012']  # Labels

# Normalizing numerical features
feature_scaler = StandardScaler()
X_sample_scaled = feature_scaler.fit_transform(X_sample)

# PCA for dimensionality reduction
pca = PCA(n_components=5)
X_sample_reduced = pca.fit_transform(X_sample_scaled)

# Splitting the sampled data into training and testing sets
x_train_sample, x_test_sample, y_train_sample, y_test_sample = train_test_split(X_sample_reduced, Y_sample, test_size=0.2, random_state=101)

# Balancing the training data using SMOTE
smote = SMOTE(random_state=101)
x_train_sample_res, y_train_sample_res = smote.fit_resample(x_train_sample, y_train_sample)

# Define the model
model = RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=101, n_jobs=-1)

# Parameter grid for GridSearchCV with reduced ranges
param_grid = {
    'n_estimators': [50, 100],  # Reduced range
    'max_depth': [10, 20],  # Reduced range
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

# Grid search for the best parameters for RandomForest
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='recall', cv=2, n_jobs=-1)
grid_search.fit(x_train_sample_res, y_train_sample_res)

# Output the best parameters and score for RandomForest
best_parameters = grid_search.best_params_
print("RandomForest Best parameters: ", best_parameters)
best_result = grid_search.best_score_
print("RandomForest Best result: ", best_result)

# Dividing dataset into label and feature
X = dataset.drop('Diabetes_012', axis=1)  # Features
Y = dataset['Diabetes_012']  # Labels

# Normalizing numerical features
X_scaled = feature_scaler.fit_transform(X)

# PCA for dimensionality reduction
X_reduced = pca.fit_transform(X_scaled)

# Splitting the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X_reduced, Y, test_size=0.2, random_state=101)

# Balancing the training data using SMOTE
x_train_res, y_train_res = smote.fit_resample(x_train, y_train)

# Build RandomForest using the best parameters
final_model = RandomForestClassifier(
    n_estimators=best_parameters['n_estimators'],
    max_depth=best_parameters['max_depth'],
    min_samples_split=best_parameters['min_samples_split'],
    min_samples_leaf=best_parameters['min_samples_leaf'],
    criterion='entropy',
    max_features='sqrt',
    random_state=101,
    n_jobs=-1
)

#Fit the dataset in the model
final_model.fit(x_train_res, y_train_res)
y_pred = final_model.predict(x_test)

#Print the evaluation matrics
print("RandomForest Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("RandomForest Classification Report:\n", classification_report(y_test, y_pred))
print(f"RandomForest Accuracy: {accuracy_score(y_test, y_pred)}")

# Predicting on the entire dataset
y_full_pred = final_model.predict(X_reduced)

#Print the evaluation matrics
print("RandomForest Full Dataset Confusion Matrix:\n", confusion_matrix(Y, y_full_pred))
print("RandomForest Full Dataset Classification Report:\n", classification_report(Y, y_full_pred))
print(f"RandomForest Full Dataset Accuracy: {accuracy_score(Y, y_full_pred)}")

# Define the model pipeline
model = Pipeline([
    ('classification', AdaBoostClassifier(random_state=101))
])

# Parameter grid for GridSearchCV with reduced ranges
param_grid = {
    'classification__n_estimators': [2, 3, 4, 5, 10, 20, 30, 40, 50]
}

# Grid search for the best parameters for AdaBoost
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='recall', cv=2, n_jobs=-1)
grid_search.fit(x_train_sample_res, y_train_sample_res)

# Output the best parameters and score for AdaBoost
best_parameters = grid_search.best_params_
print("AdaBoost Best parameters: ", best_parameters)
best_result = grid_search.best_score_
print("AdaBoost Best result: ", best_result)

# split dataset into dependent and independent variables
X = dataset.drop('Diabetes_012', axis=1)  # Features
Y = dataset['Diabetes_012']  # Labels

# Normalizing numerical features
X_scaled = feature_scaler.fit_transform(X)

# PCA for dimensionality reduction
X_reduced = pca.fit_transform(X_scaled)

# Splitting the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X_reduced, Y, test_size=0.2, random_state=101)

# Balancing the training data using SMOTE
x_train_res, y_train_res = smote.fit_resample(x_train, y_train)

# build AdaBoost using the best parameters
final_model = AdaBoostClassifier(
    n_estimators=best_parameters['classification__n_estimators'],
    random_state=101
)

#fit the data into the model
final_model.fit(x_train_res, y_train_res)
y_pred = final_model.predict(x_test)

#print evaluation matrics
print("AdaBoost Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("AdaBoost Classification Report:\n", classification_report(y_test, y_pred))
print(f"AdaBoost Accuracy: {accuracy_score(y_test, y_pred)}")

# Predicting on the entire dataset
y_full_pred = final_model.predict(X_reduced)

#print evaluation matrics
print("AdaBoost Full Dataset Confusion Matrix:\n", confusion_matrix(Y, y_full_pred))
print("AdaBoost Full Dataset Classification Report:\n", classification_report(Y, y_full_pred))
print(f"AdaBoost Full Dataset Accuracy: {accuracy_score(Y, y_full_pred)}")

# Feature importance
try:
    featimp = pd.Series(final_model.feature_importances_, index=[f'PC{i+1}' for i in range(pca.n_components_)]).sort_values(ascending=False)
    print("Feature Importances:\n", featimp)
except AttributeError:
    print("The selected model does not provide feature importances.")