# -*- coding: utf-8 -*-
"""Logistic Regression

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16KB5kHGLoceCNOBaDzSwHnkG-kOalDED
"""

# #Import libraries
# import pandas as pd
# from sklearn.model_selection import train_test_split, RandomizedSearchCV
# from sklearn.feature_selection import SelectKBest, chi2
# from sklearn.linear_model import LogisticRegression
# from scipy.stats import loguniform
# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# # Read the CSV file into a DataFrame
# df = pd.read_csv("diabetes_012_health_indicators_BRFSS2015.csv")

# # Map Diabetes_012 to binary classification (0: No Diabetes, 1: Diabetes)
# df['Diabetes_012'] = df['Diabetes_012'].map({0: 0, 1: 0, 2: 1})

# # Create feature matrix X and target variable y
# X = df.drop('Diabetes_012', axis=1)
# y = df['Diabetes_012']

# # Stratified sampling (10% of the data)
# X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.1, stratify=y, random_state=101)

# # Split the sampled data into training and testing sets
# x_train, x_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=101)

# # Feature Selection using SelectKBest (chi2 works with non-negative values only)
# selector = SelectKBest(chi2, k=10)
# x_train_selected = selector.fit_transform(x_train, y_train)
# x_test_selected = selector.transform(x_test)

# # Create and fit the logistic regression model
# model = LogisticRegression(max_iter=1000, random_state=101)

# # Hyperparameter tuning
# param_dist = {'C': loguniform(0.001, 10)}  # Log-uniform distribution for C
# random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, scoring='recall', cv=5, n_jobs=-1, random_state=101)
# random_search.fit(x_train_selected, y_train)

# # Get the best model
# best_model = random_search.best_estimator_
# best_score = random_search.best_score_

# # Step: Predict on the test set and evaluate the model
# y_pred = best_model.predict(x_test_selected)

# best_params = random_search.best_params_
# conf_matrix = confusion_matrix(y_test, y_pred)
# class_report = classification_report(y_test, y_pred)
# accuracy = accuracy_score(y_test, y_pred)

# print("Best parameters:", best_params)
# print("Best recall score:", best_score)
# print("Confusion Matrix:\n", conf_matrix)
# print("Classification Report:\n", class_report)
# print("Accuracy:", accuracy)

# import pandas as pd
# from sklearn.model_selection import train_test_split, RandomizedSearchCV
# from sklearn.feature_selection import SelectKBest, chi2
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
# from sklearn.utils import class_weight
# from imblearn.over_sampling import SMOTE
# from scipy.stats import loguniform

# # Read the CSV file
# df = pd.read_csv("diabetes_012_health_indicators_BRFSS2015.csv")

# # Map Diabetes_012 column to binary classification (0: No Diabetes, 1: Diabetes)
# df['Diabetes_012'] = df['Diabetes_012'].map({0: 0, 1: 0, 2: 1})

# # Create feature matrix X and target variable y
# X = df.drop('Diabetes_012', axis=1)
# y = df['Diabetes_012']

# # Stratified sampling to get 10% of the data
# X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.1, stratify=y, random_state=101)

# # Oversample using SMOTE
# smote = SMOTE(random_state=101)
# x_train_res, y_train_res = smote.fit_resample(X_sample, y_sample)

# # Feature selection using SelectKBest (chi2 works with non-negative values only)
# selector = SelectKBest(chi2, k=10)
# x_train_selected = selector.fit_transform(x_train_res, y_train_res)

# # Create and fit the logistic regression model, using class_weight='balanced'
# model = LogisticRegression(max_iter=1000, random_state=101, class_weight='balanced')

# # Hyperparameter tuning using RandomizedSearchCV
# param_dist = {'C': loguniform(0.001, 10)}  # Log-uniform distribution for C
# random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=20, scoring='recall', cv=5, n_jobs=-1, random_state=101)
# random_search.fit(x_train_selected, y_train_res)

# # Get the best model
# best_model = random_search.best_estimator_

# # Print the best parameters and best score
# best_params = random_search.best_params_
# best_score = random_search.best_score_

# print("Best parameters:", best_params)
# print("Best recall score:", best_score)

# # Feature selection on the entire dataset
# X_selected = selector.transform(X)

# # Predict using the best model
# y_pred_full = best_model.predict(X_selected)

# # Calculate and print the confusion matrix and classification report
# conf_matrix_full = confusion_matrix(y, y_pred_full)
# class_report_full = classification_report(y, y_pred_full)
# accuracy_full = accuracy_score(y, y_pred_full)

# print("Confusion Matrix on full dataset:\n", conf_matrix_full)
# print("Classification Report on full dataset:\n", class_report_full)
# print("Accuracy on full dataset:", accuracy_full)

# import pandas as pd
# from sklearn.model_selection import train_test_split, RandomizedSearchCV
# from sklearn.feature_selection import SelectKBest, chi2
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
# from sklearn.utils import class_weight
# from imblearn.over_sampling import SMOTE
# from scipy.stats import loguniform

# # Read the CSV file
# df = pd.read_csv("diabetes_012_health_indicators_BRFSS2015.csv")

# # Map Diabetes_012 column to binary classification (0: No Diabetes, 1: Diabetes)
# df['Diabetes_012'] = df['Diabetes_012'].map({0: 0, 1: 0, 2: 1})

# # Create feature matrix X and target variable y
# X = df.drop('Diabetes_012', axis=1)
# y = df['Diabetes_012']

# # Stratified sampling to get 10% of the data
# X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.1, stratify=y, random_state=101)

# # Oversample using SMOTE
# smote = SMOTE(random_state=101)
# x_train_res, y_train_res = smote.fit_resample(X_sample, y_sample)

# # Feature selection using SelectKBest (chi2 works with non-negative values only)
# selector = SelectKBest(chi2, k=10)
# x_train_selected = selector.fit_transform(x_train_res, y_train_res)

# # Create and fit the logistic regression model, using class_weight='balanced'
# model = LogisticRegression(max_iter=1000, random_state=101, class_weight='balanced')

# # Hyperparameter tuning using RandomizedSearchCV
# param_dist = {'C': loguniform(0.001, 10)}  # Log-uniform distribution for C
# random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=50, scoring='recall', cv=5, n_jobs=-1, random_state=101)
# random_search.fit(x_train_selected, y_train_res)

# # Get the best model
# best_model = random_search.best_estimator_

# # Print the best parameters and best score
# best_params = random_search.best_params_
# best_score = random_search.best_score_

# print("Best parameters:", best_params)
# print("Best recall score:", best_score)

# # Feature selection on the entire dataset
# X_selected = selector.transform(X)

# # Predict using the best model
# y_pred_full = best_model.predict(X_selected)

# # Calculate and print the confusion matrix and classification report
# conf_matrix_full = confusion_matrix(y, y_pred_full)
# class_report_full = classification_report(y, y_pred_full)
# accuracy_full = accuracy_score(y, y_pred_full)

# print("Confusion Matrix on full dataset:\n", conf_matrix_full)
# print("Classification Report on full dataset:\n", class_report_full)
# print("Accuracy on full dataset:", accuracy_full)

#Importing Libraries
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.utils import class_weight
from imblearn.over_sampling import SMOTE
from scipy.stats import loguniform

# Read the CSV file
df = pd.read_csv("diabetes_012_health_indicators_BRFSS2015.csv")

# Map Diabetes_012 column to binary classification (0: No Diabetes, 1: Diabetes)
df['Diabetes_012'] = df['Diabetes_012'].map({0: 0, 1: 0, 2: 1})

# splitting dataset into dependent(X) and independent variable(Y).
X = df.drop('Diabetes_012', axis=1)
y = df['Diabetes_012']

# Stratified sampling to get 10% of the data
X_sample, _, y_sample, _ = train_test_split(X, y, train_size=0.1, stratify=y, random_state=101)

# Oversample using SMOTE
smote = SMOTE(random_state=101)
x_train_res, y_train_res = smote.fit_resample(X_sample, y_sample)

# Feature selection
selector = SelectKBest(chi2, k=10)
x_train_selected = selector.fit_transform(x_train_res, y_train_res)

# Create and fit the logistic regression model
model = LogisticRegression(max_iter=1000, random_state=101)

# Hyperparameter tuning using RandomizedSearchCV
param_dist = {'C': loguniform(0.001, 10)}
random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=20, scoring='recall', cv=5, n_jobs=-1, random_state=101)
random_search.fit(x_train_selected, y_train_res)

# build the best model
best_model = random_search.best_estimator_

# Print the best parameters and best score
best_params = random_search.best_params_
best_score = random_search.best_score_

print("Best parameters:", best_params)
print("Best recall score:", best_score)

# Scaling the X
X_selected = selector.transform(X)

# Predict using the best model
y_pred_full = best_model.predict(X_selected)

# Print the confusion matrix and classification report
conf_matrix_full = confusion_matrix(y, y_pred_full)
class_report_full = classification_report(y, y_pred_full)
accuracy_full = accuracy_score(y, y_pred_full)

print("Confusion Matrix on full dataset:\n", conf_matrix_full)
print("Classification Report on full dataset:\n", class_report_full)
print("Accuracy on full dataset:", accuracy_full)